# `쿠버네티스 내부 이해`

## 목차

<br>

---
---

<br>

## 서론

쿠버네티스 내부를 살펴보자!!

<br>

---
---

<br>

## 아키텍처 이해

**컨트롤 플레인 구성요소**

클러스터 상태를 저장하고 관리하지만 애플리케이션 컨테이너를 직접 실행하는 것은 아니다.

- etcd 분산 저장 스토리지
- API 서버
- 스케줄러
- 컨트롤러 매니저

<br>

**워커 노드에서 실행하는 구성 요소**

컨테이너를 실행하는 작업은 각 워커 노드에서 실행되는 구성 요소가 담당한다.

- Kubelet
- 쿠버네티스 서비스 프록시(kube-proxy)
- 컨테이너 런타임(Docker, rkt 외 기타)

<br>

**애드온 구성 요소**

클러스터에서 여러 기능을 제공하기 위한 추가적인 구성 요소들이다.

- k8s DNS 서버
- 대시보드
- Ingress Controller
- 힙스터
- Container Network Interface Plugin
- 등등

---

<br>

### 쿠버네티스 구성 요소의 분산 특성

k8s 시스템 구성 요소는 오직 API 서버하고만 통신한다.

**API 서버는 etcd와 통신하는 유일한 구성요소다.**

API 서버와 다른 구성 요소 사이의 통신은 대부분 구성 요소에서 시작하지만, 

kubectl을 이용해 로그를 가져오거나 등의 경우는 API 서버가 Kubelet에 접속한다.

***추가적으로,*** etcd와 API 서버는 여러 인스턴스를 동시에 활성화해 작업을 병렬로 수행할 수 있지만,

스케줄러와 컨트롤러 매니저는 하나의 인스턴스만 활성화되고 **나머지는 대기** 상태에 있게 된다.

<br>

---

<br>

### k8s가 etcd를 사용하는 방법

이 책에서 생성한 모든 오브젝트는 API 서버가 다시 시작되거나 실패하더라도 유지하기 위해 매니페스트가 영구적으로 저장될 필요가 있다.

이를 위해 쿠버네티스는 빠르고, 분산해서 저장되며, 일관된 key-value 저장소를 제공하는 etcd를 사용한다.

API 서버만이 etcd와 직접적으로 통신하는 유일한 구성 요소이기 때문에 강력한 낙관적 잠금 시스템 뿐만 아니라 유효성을 검사하는 등의 이점을 얻을 수 있다.

**etcd 톺아보기**

etcd의 각 키는 다른 키를 포함하는 디렉터리이거나 유지되기 떄문에 키들이 디렉터리 구조로 그룹을 이루고 있다고 생각할 수 있다.

k8s는 모든 데이터를 /registry 아래에 저장한다.

```bash
# 조회
$ etcdctl ls /registry

# pods 디렉터리 안의 키 조회
$ etcdctl ls /registry/pods

# get으로 항목에 저장된 내용 조회
$ etcdctl get /registry/pods/default/kubia-123456
```

<br>

**추가적으로** 쿠버네티스는 다른 모든 구성 요소가 API 서버를 통하도록 하여 오류가 발생할 가능성을 줄이고 항상 일관성을 갖도록 했다.

또한 API 서버는 저장소에 기록된 데이터가 항상 유효하고 데이터 변경이 올바른 권한을 가진 클라이언트에 의해서만 수행되도록 한다.

<br>

**클러스터링된 etcd의 일관성 보장**

고가용성을 보장하기 위해 두 개 이상의 etcd 인스턴스를 실행하는 것이 일반적이다.

어떻게 일관성을 유지하는가? **RAFT 합의 알고리즘**을 통해 각 노드 상태가 대다수의 노드가 동의하는 현재 상태이거나 이전에 동의된 상태 중에 하나임을 보장한다.

(RAFT 알고리즘을 살펴보면 알겠지만 과반을 이용하기 때문에, ***홀수 개수***로 etcd 인스턴스를 배포해야 한다.)

<br>

***추가적으로,*** 대규모 etcd 클러스터에서는 일반적으로 5대 혹은 7대 노드면 충분하다고 한다.

<br>

---

<br>

### API 서버의 기능

클러스터 상태를 조회하고 변경하기 위해 RESTful API로 CRUD 인터페이스를 제공하며 상태는 etcd 안에 저장한다.

오브젝트를 etcd에 저장하는 일관된 방법을 제공하는 것 뿐만 아니라, 오브젝트 유효성 검사 작업도 수행하기 때문에 잘못 설정된 오브젝트를 저장할 수 없다.

또한 동시에 업데이트가 발생하더라도 다른 클라이언트에 의해 오브젝트의 변경 사항이 재정의되지 않는다.(낙관적 잠금)

API 서버가 내부적으로 어떻게 돌아가는지는 아래의 그림을 한 번 보자.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038158-a1634c11-8f18-418d-80f0-7fd8c28e5c00.jpeg">

<br>

1. API 서버는 인증 플러그인을 통해 클라이언트 인증서 혹은 정보를 가져온다.

2. API 서버는 인증 플러그인에서 넘겨준 데이터를 통해 인가 플러그인에서 요청한 리소스에 대한 권한이 있는지 확인한다.

3. 요청이 어드미션 컨트롤(Admission Control)로 보내진 다음, 리소스를 수정하거나 거부한다. **단, 데이터를 읽는 요청이라면 어드미션 컨트롤을 거치지 않는다.**

4. 요청이 모든 어드미션 컨트롤 플러그인을 통과하면, API 서버는 오브젝트의 유효성을 검증하고 etcd에 저장한 다음 클라이언트에 응답을 반환한다.

**참고로,** 어드미션 컨트롤 플러그인은 이런 것들이 있다.

- AlwaysPullImage: 파드의 imagePullPolicy를 Always로 변경해 파드가 배포될 때마다 이미지를 항상 강제로 가져오도록 재정의한다.
- ServiceAccount: 명시적으로 지정하지 않을 경우 default 서비스 어카운트를 적용한다.
- NamespaceLifecycle: 삭제되는 과정에 있는 네임스페이스와 존재하지 않는 네임스페이스 안에 파드가 생성되는 것을 방지한다.
- ResourceQuota: 특정 네임스페이스 안에 있는 파드가 해당 네임스페이스에 할당된 CPU와 메모리 만을 사용하도록 강제한다.

<br>

---

<br>

## API 서버가 리소스 변경을 클라이언트에 통보하는 방법

API 서버는 컨트롤러에 무엇을 해야 하는지 알려주지 않고, 배포된 리소스의 변경 사항을 관찰할 수 있도록 하면 된다.

클라이언트는 API 서버에 HTTP 연결을 맺고 변경 사항을 감지하며, 이를 통해 클라이언트는 감시 대상 오브젝트의 변경을 알 수 있는 스트림을 받는다.

오브젝트가 갱신될 때마다, 서버는 오브젝트를 감시하고 있는 연결된 모든 클라이언트에게 오브젝트의 새로운 버전을 보낸다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038175-2241ae3f-f9b6-467d-94b5-3e33f6a367b7.jpeg">

<br>

**보너스**

kubectl을 통해 리소스 변경을 감시할 수 있다.

```bash
$ kubectl get pods -o yaml --watch
```

<br>

---

<br>

### 스케줄러 이해

API 서버의 감시 메커니즘을 통해 새로 생성될 파드를 기다리고 있다가 할당된 노드가 없는 새로운 파드를 노드에 할당하기만 한다.

스케줄러는 선택도니 노드에 파드를 실행하도록 지시하지 않고, API 서버로 파드 정의를 갱신한다.

API 서버는 감시 메커니즘을 통해 Kubelet에 파드가 스케줄링 된 것을 통보하고, 대상 노드의 Kubelet이 파드의 컨테이너를 생성하고 실행한다.

파드에 가장 적합한 노드를 어떻게 선택할까? 아래에서 알아보도록 하자.

<br>

***기본적인 스케줄링 알고리즘***

- 모든 노드 중에서 파드를 스케줄링할 수 있는 노드 목록을 필터링한다.
- 수용 가능한 노드의 우선순위를 정하고 점수가 높은 노드를 선택한다. 만약 여러 노드가 같은 최상위 점수를 가지고 있다면, 라운드-로빈을 사용한다.

<br>

**수용 가능한 노드 찾기**

스케줄러는 미리 설정된 조건 함수(predicate function) 목록을 노드에 전달한다.

- 노드가 하드웨어 리소스에 대한 파드 요청을 충족시킬 수 있는가?
- 노드에 리소스가 부족한가?
- 파드를 특정 노드(이름)로 스케줄링하도록 요청한 경우에, 해당 노드인가?
- 노드가 파드 정의 안에 있는 노드 셀렉터와 일치하는 레이블을 가지고 있는가?
- 파드가 특정 호스트 포트에 할당되도록 요청한 경우 해당 포트가 이 노드에서 이미 사용 중인가?
- 파드 요청이 특정한 유형의 볼륨을 요청하는 경우 이 노드에서 해당 볼륨을 파드에 마운트할 수 있는가? 아니면 이 노드에 있는 다른 파드가 이미 같은 볼륨을 사용하고 있는가?
- 파드가 노드의 테인트(taints)를 허용하는가? => 나중에 설명한다.
- 파드가 노드의 파드의 어피니티(affinity), 안티 어피니티(anti-affinity) 규칙을 지정했는가? => 나중에 설명한다.

이 모든 검사를 통과해야 노드가 ㅂ파드를 수용할 수 있다.

<br>

**파드에 가장 적합한 노드 선택**

모든 노드가 파드를 실행할 수는 있지만, 특정 노드가 다른 노드보다 나은 선택일 수 있다.

한 노드가 10개의 파드를 실행하고 있고, 두 번째 노드가 파드를 실행하고 있지 않다면?

때에 따라 다를 수 있는 문제다.

<br>

**고급 파드 스케줄링**

파드의 레플리카가 여러 개인 경우를 가정해보자.

한 노드에 스케줄링 하는 것보다 노드가 실패할 경우를 상정해서 많은 노드에 분산시키는 것이 이상적이다.

스케줄러 없이 k8s 클러스터를 실행할 수도 있지만, 이 경우에는 스케줄링을 수동으로 실행해야 한다.

<br>

**다중 스케줄러 사용**

여러 개의 스케줄러를 실행할 수도 있는데, 파드 정의 안에 `scheduleName`이라는 속성에 스케줄러를 지정한다.

<br>

---

<br>

### 컨트롤러 매니저에서 실행되는 컨트롤러들

API 서버로 배포된 리소스에 지정된 대로 시스템을 원하는 상태로 수렴되도록 하는 역할을 한다.

다양한 조정(reconciliation) 작업을 수행하는 여러 컨트롤러가 하나의 컨트롤러 매니저 프로세스에서 실행된다.

물론, 필요한 경우 별도의 프로세스로 분할해서 구현을 교체할 수 있다. 목록은 다음과 같다.

- 레플리케이션 매니저(레플리케이션컨트롤러 리소스의 컨트롤러)
- 레플리카셋, 데몬셋, 잡 컨트롤러
- 디플로이먼트 컨트롤러
- 스테이트풀셋 컨트롤러
- 노드 컨트롤러
- 서비스 컨트롤러
- 엔드포인트 컨트롤러
- 네임스페이스 컨트롤러
- PV 컨트롤러
- 그 외 여러가지

리소스는 클러스터에 어떤 것을 실행해야 하는지 기술하는 반면, 

컨트롤러는 리소스를 배포함에 따라 실제 작업을 수행하는 활성화된(active) k8s 구성 요소다. 

<br>

**컨트롤러의 역할과 동작 방식 이해**

컨트롤러는 모두 API 서버에서 리소스가 변경되는 것을 감시하고 각 변경 작업(새 오브젝트 생성, 이미 있는 오브젝트의 갱신 혹은 삭제)을 수행한다.

일반적으로 컨트롤러는 조정 루프를 실행해, 실제 상태를 원하는 상태로 조정하고 새로운 상태를 리소스의 status에 기록한다.

감시 메커니즘을 이용해 변경 사항을 통보받지만 **모든 이벤트를 놓치지 않고 받는다는 것을 보장하진 않기 때문에**,

정기적으로 목록을 가져오는 작업을 수행해 누락된 이벤트가 없는지 확인해야 한다.

컨트롤러는 서로 직접 대화하지 않고 심지어 존재하는지 여부도 모른다.

더 구체적인 내용을 살펴보고 싶다면, 소스 코드를 직접 살펴보자.

이제부터는 간략하게 컨트롤러를 설명하고자 한다.

<br>

***레플리케이션 매니저***

레플리케이션컨트롤러 리소스를 활성화하는 컨트롤러를 레플리케이션 매니저라고 한다.

매번 루프를 돌 때마다 컨트롤러는 파드 셀렉터와 일치하는 파드의 수를 찾고 이를 원하는 레플리카 수와 비교한다.

컨트롤러는 감시 메커니즘을 통해 레플리카 수와 매칭된 실제 파드 수에 영향을 주는 변화를 수신할 수 있따.

따라서 변경을 통해 컨트롤러가 호출되고 원하는 레플리카 수와 실제 레플리카 수를 다시 확인한 다음, 그에 마즌ㄴ 동작을 수행한다.

<br>

***나머지 컨트롤러에 대한 설명을 다 적을 수 있지만, 사실 레플리케이션 매니저와 거의 비슷한 동작으로 작동한다.***

***따라서 더 궁금하다면 찾아보도록 하자.***

<br>

이것 한 가지만 기억하자.

***모든 컨트롤러는 API 서버로 API 오브젝트를 제어하며, 어떤 컨트롤러도 Kubelet과 직접 통신하거나 어떠한 명령도 내리지 않는다.***

<br>

---

<br>

### Kubelet이 하는 일

Kubelet과 서비스 프록시는 실제 파드 컨테이너가 실행되고 있는 워커 노드에서 실행된다.

Kubelet은 워커 노드에서 일행하는 모든 것을 담당하는 구성 요소다.

Kubelet은 다음과 같은 일들을 한다.

1. Kubelet이 실행 중인 노드를 노드 리소스로 만들어서 API 서버에 등록한다.

2. API 서버를 지속적으로 모니터링해 해당 노드에 파드가 스케줄링되면, 파드의 컨테이너를 시작한다.

3. 실행 중인 컨테이너를 계속 모니터링하면서 상태, 이벤트, 리소스 사용량을 API 서버에 보고한다.

4. 이외에도 Container Liveness Probe를 실행하기도 하며, 프로브가 실패하면 컨터이너를 재시작한다. 또한, API 서버에서 파드가 삭제되면 컨테이너를 정지하고 파드가 종료된 것을 서버에 통보한다.

<br>

***추가적으로*** API 서버 없이도 Kubelet을 통해 파드를 실행시킬 수도 있다.

<br>

---

<br>

### k8s 서비스 프록시의 역할

Kubelet 외에도, 모든 워커 노드는 클라이언트가 k8s API로 정의한 서비스에 연결할 수 있도록 해주는 kube-proxy도 같이 실행한다.

kube-proxy는 서비스의 IP와 포트로 들어온 접속을 서비스를 지원하는 파드 중 하나와 연결시켜 준다.

서비스가 둘 이상의 파드에서 지원되는 경우, 파드 간 로드 밸런싱도 수행한다.

<br>

***왜 이름이 프록시지?***

kube-proxy는 iptables(패킷 처리 로직)를 필요에 따라 명백하게 redirection 되는 가상 IP 주소를 정의하기 위해 사용한다.

즉 클라이언트가 VIP(가상 IP 주소)에 연결하면, 트래픽이 자동으로 적절한 엔드포인트로 전송되는 것이다.

초기에는 실제로 프록시 서버를 거쳤지만 현재는 패킷을 무작위로 선택해 백엔드로 전달한다고 한다.(iptables 프록시 모드)

**관련 링크** : https://kubernetes.io/ko/docs/concepts/services-networking/service/#ips-and-vips

<br>

---

<br>

### 쿠버네티스 애드온

항상 필요한 것은 아니지만 활성화할 수 있는 기능이 있다.

k8s 서비스의 DNS 조회, 여러 HTTP 서비스를 단일 외부 IP 주소로 노출하는 기능 등이다.

이러한 구성 요소는 애드온으로 제공되고, YAML 매니페스트를 API 서버에 게시해 파드로 배포된다.

<br>

---
---

<br>

## 컨트롤러가 협업하는 방법

k8s 클러스터를 구성하는 모든 구성 요소를 알아봤다.

이제 디플로이먼트 리소스를 생성하고 파드 컨테이너가 시작하기까지 일어나는 모든 일들을 알아보자.

<br>

---

<br>

### 관련된 구성 요소 이해

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038174-22b1f4a9-e1ab-4cfd-aaa2-764ea524b191.jpeg">

<br>

---

<br>

### 이벤트 체인

디플로이먼트 매니페스트를 준비해 쿠버네티스에 게시한다고 가정하자.

1. kubectl은 매니페스트를 HTTP POST 요청으로 API 서버에 전송한다. API 서버는 디플로이먼트 정의를 검증하고 etcd에 저장한 후에 kubectl에 응답을 돌려준다.

2. 감시 메커니즘을 이용해 디플로이먼트 목록을 관찰하던 모든 API 서버 클라이언트는 새 리소스가 생성되면 즉시 통보를 받는다. (디플로이먼트 컨트롤러 포함)
   
3. 디플로이먼트 컨트롤러는 현재 디플로이먼트 정의를 이용해 레플리카셋을 생성한다.(당연히 k8s API로 새로운 레플리카 셋을 만든다.)

4. 감시 메커니즘을 이용해 레플리카셋 리소스의 생성, 수정, 삭제를 감시하는 레플리카셋 컨트롤러가 새로 생성된 레플리카셋을 확인한다.

5. 그런 다음 레플리카셋 컨트롤러는 레플리카셋의 파드 템플릿을 기반으로 파드 리소스를 생성한다.

6. 새로 생성된 파드는 etcd에 저장되었지만 아직 연결된 노드가 없다. 스케줄러는 `nodeName`이 없는 파드를 발견한다.

7. 그리고 스케줄러는 가장 적합한 노드를 선정해 노드에 할당한다. 현재까지 이 모든 일은 **컨트롤 플레인**에서 일어났다.

8. 파드가 특정 노드로 스케줄링되었음을 감시 메커니즘으로 Kubelet이 알게 되고, 파드 정의를 검사한 다음 컨테이너 런타임(Docker 등)에 파드의 컨테이너를 시작하도록 지시한다.

***컨테이너가 시작되었다!!***

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038172-2efb1dae-98bd-4788-97a3-adaaede237b9.jpeg">

<br>

---

<br>

### 클러스터 이벤트 관찰

컨트롤 플레인 구성요소와 Kubelet은 이러한 작업을 수행할 때 이벤트 리소스를 만들어 API 서버로 이벤트를 발송한다. 

클러스터에서 무슨 일이 일어나는지 보고 싶다고? 아래의 명렁어를 실행해보자.

```bash
$ kubectl get events --watch
```

<br>

---
---

<br>

## 실행 중인 파드에 관한 이해

파드가 컨테이너 하나만 갖고 있을 때 `Kubelet`이 단지 이 컨테이너 하나만 실행할까?

파드를 실행하고 나서 해당 파드가 실행되는 노드에 ssh로 접속한 다음,

```bash
$ docker ps
```
를 실행해보자.

<br>

아니, 다른 컨테이너도 있는데 우리가 실행한 파드 관련 컨테이너보다 몇 초 일찍 생성되었다.

COMMAND는 "/pause"라고 되어 있다. 얘는 뭐지?

<br>

***퍼즈(pause)*** 컨테이너는 파드의 모든 컨테이너를 함께 담고 있는 컨테이너이다.

파드의 모든 컨테이너가 동일한 네트워크와 리눅스 네임스페이스를 공유하는 방법을 기억하는가?

퍼즈 컨테이너는 이런 네임스페이스르 모두 보유하는 게 유일한 목적인 인프라 컨테이너이다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038171-08add3ca-12a8-438c-a9d2-62dcd3144e2f.jpeg">

<br>

실제 애플리케이션 컨테이너는 종료되고 다시 시작할 수 있기 때문에 재시작할 때 이전과 동일한 네임스페이스의 일부가 되어야 한다.

퍼즈 컨테이너의 라이프 사이클은 파드 라이프 사이클과 같기 때문에 이것이 가능하다.

<br>

---
---

<br>

## 파드 간 네트워킹

각 파드가 고유한 IP 주소를 가지고 다른 모든 파드와 NAT 없이 플랫 네트워크로 서로 통신할 수 있다는 것을 우리는 안다.

도대체 쿠버네티스는 이를 어떻게 할까?

정답은 네트워크는 k8s 자체가 아닌, ***시스템 관리자 또는 컨테이너 네트워크 인터페이스(CNI) 플러그인***에 의해 제공되기 때문이다.

<br>

---

<br>

### 그러니깐 이런 느낌

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038170-9739cedb-66db-4e29-86e5-6f7433feff1a.jpeg">

<br>

---

### 자세히 알아보자

파드 IP 주소와 네트워크 네임스페이스가 퍼즈 컨테이너에 의해 설정되고 유지되는 것을 우리는 안다.

파드의 컨테이너는 해당 네트워크 네임스페이스를 사용한다.

네트워크 인터페이스를 생성하는 방법과 생성한 인터페이스를 모든 다른 파드 인터페이스에 연결하는 방법을 보자.

<br>

***동일 노드 파드 간 통신 활성화***

퍼즈 컨테이너가 시작되기 전에, 컨테이너를 위한 **가상 이더넷 인터페이스 쌍**이 생성된다.

한 쪽 인터페이스는 호스트의 네임스페이스에 남아있고, 다른 쪽 인터페이스는 컨테이너의 네트워크 네임스페이스 안으로 옮겨져 이름이 `eth0`으로 변경된다.

**한 쪽 인터페이스로 들어가면 다른 쪽으로 나오는 구조이다.**

호스트의 네트워크 네임스페이스에 있는 인터페이스는 컨테이너 런타임이 사용할 수 있도록 설정된 네트워크 브리지에 연결된다.

컨테이너 안의 `eth0` 인터페이스는 브리지의 주소 범위 안에서 IP를 할당받는다.

컨테이너 내부에서 실행되는 애플리케이션은 eth0 인터페이스로 전송하면, 호스트 네임스페이스의 다른 쪽 인터페이스로 나와 브리지로 전달된다.

이는 브리지에 연결된 모든 네트워크 페이스에서 수신할 수 있다는 것을 의미한다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038169-61ce890d-021e-40be-a65f-7bcb62643ad1.jpeg">

<br>

***서로 다른 노드에서 파드 간의 통신 활성화***

서로 다른 노드 사이에서 브리지를 연결하는 방법은 여러가지가 있따.

- 오버레이 네트워크
- 언더레이 네트워크
- 일반적인 Layer 3 라우팅

파드 IP 주소는 전체 클러스터 내에서 유일해야 하기 때문에,

노드 사이의 브리지는 겹치지 않는 주소 범위를 사용해 다른 노드에 있는 파드가 같은 IP 주소를 얻지 못해야 한다.

일단 일반적인 Layer 3 라우팅을 사용하면 노드의 물리 네트워크 인터페이스가 브리지에 연결되어 있어야 한다.

단, 아래 그림 같은 경우는 두 노드가 라우터 없이 같은 네트워크 스위치에 연결된 경우에만 동작한다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038167-69ba59f2-c65c-4d5d-80af-32844ab6d3b2.jpeg">

<br>

라우터가 중간에 있다면 파드의 IP가 프라이빗 대역에 속하기 때문에 라우터는 패킷을 삭제하기 때문이다.

따라서 이런 경우, 소프트웨어 정의 네트워크(SDN)을 사용하여 캡슐화 -> 디캡슐화 과정을 통해 패킷 형태로 대상 파드에 전달하여 해결한다.

<br>

---

<br>

### 컨테이너 네트워크 인터페이스(CNI)

컨테이너를 네트워크에 쉽게 연결하기 위해, CNI를 설치하여 사용할 수 있다.

YAML을 배포하면 되는데 여러 플러그인이 있기 때문에 찾아보고 배포하면 된다!

참고로 데몬셋과 다른 지원 리소스를 통해 배포해야 하는데, 데몬셋은 네트워크 에이전트를 모든 클러스터 노드에 배포하는데 사용된다.

<br>

---
---

<br>

## 서비스 구현 방식

서비스는 파드 집합을 길게 지속되는 안정적인 IP와 포트로 노출시킨다.

어떻게 작동하는지 알아보자.

<br>

---

<br>

### kube-proxy

서비스와 관련된 모든 것은 각 노드에서 동작하는 `kube-proxy` 프로세스에 의해 처리된다.

IP 주소는 가상이기 때문에 어떠한 네트워크 인터페이스에도 할당되지 않고 

패킷이 노드를 떠날 때 네트워크 패킷 안에 출발지 혹은 도착지 IP 주소로 표시되지 않는다.

서비스는 IP와 포트의 쌍으로 구성되고, 서비스 IP만으로는 아무것도 나타내지 않는다.

<br>

---

### kube-proxy가 iptables를 사용하는 방법

API 서버에서 서비스를 생성하면, VIP가 바로 할당된다.

곧이어 API 서버는 워커 노드에서 실행 중인 모든 kube-proxy 에이전트에 새로운 서비스가 생성됐음을 통보한다.

각 kube-proxy는 실행 중인 노드에 서비스 주소로 접근할 수 있도록 만든다.

이것은 서비스의 IP/포트 쌍으로 향하는 패킷을 가로채서, 

목적지 주소를 변경해 패킷이 서비스를 지원하는 여러 파드 중 하나로 리디렉션되도록 하는 몇 개의 iptables 규칙을 설정함으로써 이뤄진다.

kube-proxy는 API 서버에서 서비스가 변경되는 것을 감지하는 것 외에도, 엔드포인트 오브젝트가 변경되는 것을 같이 감시한다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038165-e08c04ae-2370-4077-a072-705c602a62b3.jpeg">

<br>

---
---

<br>

## 고가용성 클러스터 실행

쿠버네티스 안에서 애플리케이션을 실행하는 이유 가운데 하나는 인프라스트럭처 장애가 발생하는 경우에도

사용자의 개입 없이 혹은 제한적인 수동 개입만으로 중단 없이 계속 실행할 수 있게 해주기 때문이다.

<br>

---

### 애플리케이션 가용성 높이기

**가동 중단 시간을 줄이기 위한 다중 인스턴스 실행**

수평 확장에 속하지 않는 애플리케이션도, 레플리카 수가 1로 지정된 디플로이먼트를 사용해야 한다.

레플리카를 사용할 수 없게 되면, 새 레플리카로 빠르게 교체된다.

그러나 관련된 컨트롤러가 노드에 장애가 있음을 인지하고 새 파드 레플리카를 생성한 후에 컨테이너를 시작하는데 시간이 걸린다.

그 사이에 짧은 중단 시간이 발생하는 것은 어쩔 수 없다.

<br>

**수평 스케일링이 불가능한 애플리케이션을 위한 리더 선출 메커니즘 사용**

중단 시간 발생을 피하기 위해, 

활성 복제본과 함께 비활성 복제본을 실행해두고 빠른 임대(fast-acting lease) 혹은 리더 선출 메커니즘을 이용해 단 하나만 활성화 상태로 만들어야 한다.

메커니즘 자체가 애플리케이션에 포함될 필요는 없고, 모든 리더 선출 작업을 수행하고 활성화될 때 신호를 메인 컨테이너로 보내는 사이드카 컨테이너를 사용할 수 있다.

<br>

마지막으로 쿠버네티스 자체가 실패하는 경우를 알아보도록 하자.

---

<br>

### k8s 컨트롤 플레인 구성 요소 가용성 향상

고가용성 클러스터의 예시를 먼저 보자.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126038164-b1596aa3-04ed-453b-80aa-739f3cf154ac.jpeg">

<br>

**etcd 클러스터 실행**

etcd 클러스터에 관한 내용은 앞에서 이야기 했기 때문에 패스

<br>

**여러 서버 API 인스턴스 실행**

etcd 클러스터보다 쉽다. API 서버는 거의 stateless이기 때문에, 모든 etcd 인스턴스에 API 서버를 띄우기만 하면 된다.

<br>

**컨트롤러와 스케줄러의 고가용성 확보**

컨트롤러 매니저나 스케줄러의 여러 인스턴스를 동시에 실행하는 것은 쉬운 일이 아니다.

컨트롤러와 스케줄러는 클러스터 상태를 감시하고 상태가 변경될 때 반응해야 하는데

이런 요소의 여러 인스턴스가 동시에 실행돼 같은 동작을 수행하면, 클러스터 상태가 예상보다 더 많이 변경될 가능성이 있다.

이런 이유 때문에 **컨트롤러 매니저나 스케줄러 같은 구성 요소는 한 번에 하나의 인스턴스만 활성화되게 해야 한다.**

다행스럽게도 리더 선출 메커니즘을 자체적으로 수행한다.

***따라서, 단 하나만 활성화되고 나머지는 대기 상태에 있게 된다.***

<br>

**컨트롤 플레인 구성요소에서 사용되는 리더 선출 메커니즘 이해**

리더를 선출하기 위해 서로 직접 대화할 필요가 없다.

리더 선출 메커니즘은 API 서버에 오브젝트를 생성하는 것만으로 완전히 동작하며, 다른 리소스가 필요하지 않다.

**낙관적 동시성**을 통해 여러 인스턴스가 자신의 이름을 리소스에 기록하려고 노력하지만 단 하나만이 성공한다.

따라서 이름 기록 성공 여부에 따라 각 인스턴스는 리더인지 아닌지 알 수 있다.

리더가 되면 주기적으로 리소스를 갱신해서(default는 2초) 다른 모든 인스턴스에서 리더가 살아있음을 알 수 있도록 해야 한다.

**만약 장애가 있어서 갱신되지 않으면? 자신의 이름을 리소스에 기록해 리더가 되려고 한다!**


