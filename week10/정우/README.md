# `파드의 컴퓨팅 리소스 관리`

## 목차

<br>

---
---

<br>

## 서론

지금까지는 사용할 수 있는 CPU와 메모리의 양을 고려하지 않고 파드를 만들었다.

파드의 예상 소비량과 최대 소비량을 설정하는 것은 파드의 정의에서 매우 중요하다.

<br>

---
---

<br>

## 파드 컨테이너의 리소스 요청

파드으를 생성할 때 컨테이너가 필요로 하는 CPU와 메모리 양(requests)과 사용할 수 있는 엄격한 제한(limits)을 지정할 수 있다.

이것들은 컨테이너에 개별적으로 지정되며 파드 전체에 지정되지 않는다.

---

<br>

### 리소스를 요청하는 파드 생성하기

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      requests:
        # 200 밀리코어는 하나의 CPU 코어 시간의 1/5이다.
        # 최소 200m 이상을 요구하는 것이다.
        # 즉 1000m을 요구하게 되면 하나의 코어를 요청하게 된다.
        cpu: 200m
        # 최대 10Mi의 메모리를 사용할 수 있다.
        memory: 10Mi
```

만약 CPU 요청을 지정하지 않으면 컨테이너에서 실행 중인 프로세스에 할당되는 CPU 시간에 신경쓰지 않는다는 것과 같다.

최악의 경우 CPU 시간을 전혀 할당받지 못할 수 있다.

시간이 중요하지 않은 우선순위가 낮은 배치 작업은 괜찮지만 사용자 요청을 처리하는 컨테이너에는 분명 적합하지 않다.

```bash
$ kubectl exec -it requests-pod top
```

CPU 사용량이 20%는 넘었을 것이다. 우리가 따로 제한을 걸지 않았기 때문에 높지 않을 것이다.

제한은 뒤에서 설명한다.

<br>

---

<br>

### 리소스 요청이 스케줄링에 미치는 영향

스케줄러는 파드를 스케줄링할 때 파드의 리소스 요청 사항을 만족하는 충분한 리소스를 가진 노드만을 고려한다.

<br>

**파드가 특정 노드에 실행할 수 있는지 스케줄러가 결정하는 방법**

스케줄러는 스케줄링하는 시점에 각 개별 리소스가 얼마나 사용되는지 보지 않고, 노드에 배포된 파드들의 리소스 요청량의 전체 합만을 본다.

노드에 배포된 파드들의 리소스 요청량의 전체 합만을 본다는 것이다.

파드가 요청한 것보다 적게 사용할지라도 실제 리소스 사용량에 기반해 다른 파드를 스케줄링한다는 것은 이미 배포된 파드에 대한 보장을 깨뜨릴 수 있다.

<br>

**스케줄러가 파드를 위해 최적의 노드를 선택할 때 파드의 요청을 사용하는 방법**

다른 여러 우선순위 함수 중에서, 두 개의 우선순위함수가 요청된 리소스 양에 기반해 노드의 순위를 정한다.

**LeastRequestedPriority**와 **MostRequestedPriority**이다.

**LeastRequestedPriority*는 요청된 리소스가 낮은, 할당되지 않은 리소스의 양이 큰 노드를 선호하는 반면,

**MostRequestedPriority**는 그와 정반대로 요청된 리소스가 가장 많은 노드를 선호한다.

두 함수 모두, 요청된 리소스의 양만을 고려한다.

<br>

**노드의 용량 검사**

```bash
$ kubectl describe nodes
```

출력 결과를 보면 노드의 사용 가능한 리소스가 두 세트로 표시된다.

노드의 **capacity** 와 **allocatable** 리소스다.

**capacity**는 노드의 총 리소스를 나타내고 파드에서 모두 사용가능한 것은 아니다.

특정 리소스는 쿠버네티스와 시스템 구성 요소로 스케줄링된 것일 수 있다.

스케줄러는 오직 **allocatable** 리소스 양을 기준으로 결정한다.

2개의 CPU 코어가 있다고 가정하고, 각각 200m, 800m, 1000m을 요구하는 파드를 배포해보자.

API 서버에서 승인은 나지만 실제로 파드 상태를 살펴보면 Pending되어 있는 것을 볼 수 있다.

이유가 뭘까?

<br>

**파드가 스케줄링되지 않은 이유**

```bash
$ kubectl describe node
```

실제로 kube-system 네임스페이스의 세 파드가 명시적으로 CPU 리소스 요청한 것을 볼 수 있다.

따라서 리소스가 부족해서 스케줄링 되지 않았음을 알 수 있다.

<br>

**파드가 스케줄링될 수 있도록 리소스 해제**

파드는 적절한 양의 CPU가 남아있는 경우에만 스케줄링이 된다.

800m 짜리 파드를 삭제하고 나서 상태를 확인하면, 세 번째 파드가 동작하는 것을 볼 수 있다.

<br>

---

<br>

### CPU 요청이 CPU 시간 공유에 미치는 영향

파드 내부의 프로세스가 할 수 있는 만큼 CPU를 소비한다면 얼마의 CPU 시간을 파드가 얻게 될까?

CPU 요청은 단지 스케줄링에만 영향을 미칠 뿐 아니라 남은(미사용한) CPU 시간을 파드 간에 분배하는 방식도 결정한다.

만약 한 파드가 쉬고 있고, 다른 파드는 CPU를 최대로 사용하려고 한다면 이 파든는 전체 CPU 시간을 사용할 수 있다.

대신 두 파드 모두 CPU 시간을 필요로 한다면 CPU 시간은 조절되게 된다.

<br>

---

<br>

### 사용자 정의 리소스의 정의와 요청

k8s를 사용하면 사용자 정의 리소스를 노드에 추가하고 파드의 리소스 요청으로 사용자 정의 리소스를 요청할 수 있다.(Extended Resources)

먼저 노드 오브젝트의 capacity 필드에 값을 추가해 쿠버네티스가 사용자 정의 리소스를 인식하도록 해야한다. (kubernetes.io로 시작하지만 않으면 된다.)

파드를 생성할 떄 동일한 리소스 이름과 수량을 컨테이너 스펙의 resources.requests 필드로 지정해야 한다.

스케줄러는 요청된 양의 사용자 정의 리소스가 사용 가능한 노드에만 파드가 배포되도록 한다.

배포된 모든 파드는 당연히 할당 가능한 리소스의 단위 수를 차감시킨다.

사용자 정의 리소스의 예시로는 GPU 단위 수가 있다.

<br>

---
---

<br>

## 컨테이너에 사용 가능한 리소스 제한

컨테이너가 사용할 수 있는 최대량을 알아보자.

<br>

---

<br>

### 컨테이너가 사용 가능한 리소스 양을 엄격한 제한으로 설정

CPU는 컨테이너에서 실행 중인 프로세스에 부정적인 영향을 주지 않고 컨테이너가 사용하는 CPU 양을 조절할 수 있다.

하지만 메모리는 프로세스가 메모리를 해제하지 않는 한 가져갈 수 없다.

따라서 컨테이너에 할당되는 메모리의 최대량을 제한해야 한다.

즉, 오작동하거나 악의적인 파드 하나가 메모리를 모두 점유하고 있다면 이 파드 하나가 실제 전체 노드를 사용할 수 없게 만들 수도 있다.

***리소스 제한을 갖는 파드 생성***

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        # 최대 사용 가능 코어
        cpu: 1
        # 최대 사용 가능 메모리
        memory: 20Mi
```

***추가로,*** 따로 요청(requests)을 지정하지 않았기 때문에 limits와 동일하게 설정된다.

<br>

**리소스 제한 오버커밋**

리소스 제한은 노드 용량의 100%를 초과할 수 있다.(오버커밋될 수 있다.)

노드 리소스의 100%가 다 소진되면 특정 컨테이너는 제거돼야 한다.

<br>

---

<br>

### 리소스 제한 초과

컨테이너에서 실행 중인 프로세스가 허용된 양보다 많은 리소스를 사용하려고 하면 어떤 일이 발생할까?

CPU는 별 문제 없지만, 메모리는 프로스세가 제한보다 많은 메모리를 할당받으려 시도하면 프로세스는 종료된다.

만약 파드가 재시작되도록 설정되어 있다면 종료됐음을 알아차리지 못할 수도 있다.

하지만 메모리 제한 초과와 종료가 지속되면 쿠버네티스는 재시작 사이의 지연 시간을 증가시키면서 재시작된다.

이런 경우 STATUS에 CrashLoopBackOff가 표시된다.

간격은 20초, 40초, 80초, 160초로 증가하고 마지막에 300초로 제한된다.

컨테이너는 제한을 넘어서지 않아도 OOMKilled(Out Of Memory) 될 수 있는데, 이 부분은 뒤에서 더 알아보자.

<br>

---

<br>

### 컨테이너의 애플리케이션이 제한을 바라보는 방법

```bash
$ kubectl create -f limited-pod.yaml

$ kubectl exec -it limited-pod top
```

어라? 사용된 메모리와 사용 가능한 메모리의 양이 20MiB와는 거리가 멀고, CPU 제한도 프로세스가 사용 가능한 CPU의 50%를 사용하는 것으로 보인다.

<br>

**컨테이너는 항상 컨테이너 메모리가 아닌 노드 메모리를 본다**

top 명령은 컨테이너가 실행 중인 전체 노드의 메모리 양을 표시한다. 

컨테이너에 사용가능한 메모리의 제한을 설정하더라도 컨테이너는 이 제한을 인식하지 못한다.

특히 JVM의 경우, 최대 힙 메모리 크기를 지정하지 않으면 컨테이너에 사용 가능한 메모리 대신 호스트의 총 메모리를 기준으로 최대 힙 크기를 설정하게 된다.

따라서 JVM이 컨테이너에 설정된 메모리 제한을 초과해 OOMKilled 될 수 있다.

<br>

**컨테이너는 또한 노드의 모든 CPU 코어를 본다.**

메모리와 마찬가지로 컨테이너는 컨테이너에 설정된 CPU 제한과 상관없이 노드의 모든 CPU를 본다.

CPU 제한을 1코어로 설정하는 것은 컨테이너에 CPU 1코어만 노출하는 것이 아닌, 컨테이너가 사용할 수 있는 CPU 시간의 양을 제한하는 것이다.

그런데, 시스템의 CPU 수를 검색해 실행해야 할 작업 스레드 수를 결정하는 애플리케이션이라면?

엄청나게 많은 수의 코어를 갖는 환경에 배포하면 너무 많은 스레드가 기동돼 제한된 CPU 시간을 두고 모두 경합하게 될 수도 있다.

<br>

**이런 특수한 상황들을 어떻게 해결할지를 고민하기 보다 이런 사항들이 있다는 것을 유념해두자.**

---
---

<br>

## 파드 QoS 클래스 이해

리소스 제한은 오버커밋될 수 있으므로 노드가 모든 파드의 리소스 제한에 지정된 양의 리소스를 반드시 제공할 수는 없다고 이미 이야기를 했다.

만약 A라는 파드가 90%의 리소스를 사용하고 있는데 갑자기 파드 B가 기존보다 더 많은 리소스를 요구해서 노드가 해당 리소스를 제공할 수 없다면, 어떤 컨테이너를 종료해야 할까?

이는 상황에 따라 다르고 k8s는 스스로 적절한 결정을 내릴 수 없기 때문에 어떤 파드가 우선순위를 가지는지 지정할 방법이 필요하다.

k8s는 파드를 세 가지 QoS(Quality of Service)로 분류한다.

- BestEffort(최하위 우선순위)
- Burstable
- Guaranteed(최상위 우선순위)

<br>

---

<br>

### 파드의 QoS 정의

QoS 클래스는 파드 컨테이너의 리소스 요청과 제한의 조합에서 파생되기 떄문에 따로 정의해서 할당하지 않는다.

**BestEffort**

아무런 리소스 요청과 제한이 없는 파드에 BestEffort 클래스가 할당된다.

이런 파드에 실행 중인 컨테이너는 리소스 보장을 받지 못하고 리소스가 부족하면 제일 먼저 종료될 수 있다.

단, 메모리가 충분하다면 컨테이너는 원하는 만큼 메모리를 사용할 수 있다.

<br>

**Guaranteed**

모든 리소스를 컨테이너의 리소스 요청이 리소스 제한과 동일한 파드에게 주어진다.

조건은 다음을 충족해야 한다.

- CPU와 메모리에 리소스 요청과 제한이 모두 설정돼야 한다.
- 각 컨테이너에 설정돼야 한다.
- 리소스 요청과 제한이 동일해야 한다.

리소스 요청이 명시적으로 설정되지 않은 경우 모든 리소스에 대한 제한을 지정한다면 Guaranteed가 된다.

이런 파드의 컨테이너는 요청된 리소스 양을 얻지만 추가 리소스를 사용할 수 없다.

<br>

**Burstable**
BestEffort와 Guaranteed 사이가 Burstable이다.

Burstable 파드는 요청한 양 만큼의 리소스를 얻지만 필요하면 추가 리소스를 사용할 수 있다.


<br>

**다중 컨테이너 파드의 QoS 클래스 파악**

다중 컨테이너 파드의 경우 모든 컨테이너가 동일한 QoS를 가지면 그것이 파드의 QoS가 된다.

적어도 한 컨테이넉 다른 클래스를 가지면 파드의 QoS는 Burstable이 된다.

---

<br>

### 메모리가 부족할 때 어떤 프로세스가 종료되는지 이해

시스템이 오버커밋되면 QoS 클래스는 어떤 컨테이너를 먼저 종료할지 결정하고 해제된 리소스를 높은 우선순위의 파드에 줄 수 있다.

BestEffort 클래스가 가장 먼저 종료되고 다음은 Burstable 파드가 종료되며, 마지막으로 Guaranteed 파드는 시스템 프로세스가 메모리를 필요로 하는 경우에만 종료된다.

<br>

**동일 QoS를 갖는 컨테이너를 다루는 방법**

실행 중인 각 프로세스는 OOM 점수(Out Of Memory score)를 갖는다.

시스템은 모든 실행 주인 프로세스의 OOM 점수를 비교해 종료할 프로세스를 선정한다.

메모리 해제가 필요하면 가장 높은 점수의 프로세스가 종료된다.

OOM 점수는 두 가지 기준으로 계산된다.

- 프로세스가 소비하는 가용 메모리의 비율
- 컨테이너의 요청된 메모리와 파드의 QoS 클래스를 기반으로 한 OOM 점수 조정

즉 더 많은 메모리를 사용하더라도, 메모리 요청에 관한 사용량 비율이 높으면 그 파드를 먼저 종료한다.

<br>

---
---

<br>

## 네임스페이스별 파드에 대한 기본 요청과 제한 설정

모든 컨테이너에 리소스 요청과 제한을 설정하면 다른 컨테이너에 의해 좌지우지 되지 않는다!

<br>

---

<br>

### LimitRange 리소스 소개

모든 컨테이너에 리소스 요청과 제한을 설정하는 대신 LimitRange 리소스를 생성해 이를 수행할 수 있다.

LimitRange 리소스는 컨테이너의 각 리소스에 최소/최대 제한을 지정할 뿐만 아니라 리소스 요청을 명시적으로 지정하지 않은 컨테이너의 기본 리소스 요청을 지정한다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126868273-f7865489-021a-4e9b-8a34-20ac0a6f85f0.jpeg">

<br>

LimitRange 리소스는 LimitRanger 어드미션 컨트롤 플러그인에서 사용된다.

파드 매니페스트가 API 서버에 게시되면 LimitRanger 플러그인이 파드 스펙을 검증한다.

검증이 실패하면 매니페스트는 즉시 거부된다.

<br>

---

<br>

### LimitRange 오브젝트 생성

```yaml
apiVersion: v1
kind: KimitRange
metadata:
  name: example
spec:
  limits:
  # 파드 전체에 리소스 제한
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
  # 컨테이너 제한
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 10Mi
    default:
      cpu: 200m
      memory: 100Mi
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
    # 각 리소스 제한과 요청 간의 최대 비율
    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  # LimitRange는 PVC가 요청할 수 있는 스토리지의 최소 및 최대량을 설정할 수 있다.
  - type: PersistentVolumeClaim
    min:
      stroage: 1Gi
    max:
      storage: 10Gi
```

나중에 제한을 수정하게 되면 기존 파드 및 PVC는 다시 검증되지 않는다는 것을 유의하자.

<br>

---

<br>

### 강제 리소스 제한

제한이 설정되면 LimitRange의 최대보다 큰 CPU를 요청하는 파드를 생성 요청하면  파드가 거부되는 이유까지 포함해서 보여준다.

<br>

---

<br>

### 기본 리소스 요청과 제한 적용

제한이 지정되지 않은 컨테이너를 생성하게 되면, LimitRange 오브젝트를 통해 기본값이 자동으로 적용된다.

```bash
$ kubectl describe po kubia-manual
```

컨테이너의 요청과 제한이 LimitRange 오브젝트에서 지정한 것과 일치한다!

또 다른 네임스페이스에 다른 LimitRange 스펙을 사용한다면 그 네임스페이스의 파드는 분명 다른 요청과 제한으로 생성된다.

이처럼 관리자가 네임스페이스당 파드의 기본, 최소, 최대 리소스를 설정할 수 있게 도와준다.

<br>

---
---

<br>

## 네임스페이스의 사용가능한 총 리소스 제한하기

앞서 본 것처럼 LimitRange는 개별 파드에만 적용되지만, 클러스터 관리자는 네임스페이스에서 사용가능한 총 리소스 양을 제한할 수 있는 방법이 필요하다.

<br>

---

### 리소스쿼터 오브젝트(ResourceQuota)

리소스쿼터 어드미션 컨트롤 플러그인은 생성 중인 파드가 설정된 리소스쿼터를 초과하는지 확인한다.

만약 그런 경우 파드 생성은 거부된다.

리소스쿼터는 파드를 생성할 때 적용되므로 리소스 쿼터 오브젝트는 리소스쿼터 오브젝트를 생성한 후 생성된 파드에만 영향을 미친다.

**리소스쿼터는 네임스페이스에서 파드가 사용할 수 있는 컴퓨터 리소스 양과 PVC가 사용할 수 있는 스토리지 양을 제한한다.**

**또한 네임스페이스 안에서 사용자가 만들 수 있는 파드, 클레임, 기타 API 오브젝트의 수를 제한할 수 있다.**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-and-mem
spec:
  hard:
    # 네임스페이스에서 파드가 요청할 수 있는 최대 CPU를 400m으로 설정
    # 네임스페이스의 최대 총 CPU 제한은 600m
    requests.cpu: 400m
    requests.memory: 200Mi
    limits.cpu: 600m
    limits.memory: 500Mi
```

각 리소스의 합계를 정의하는 대신 CPU 및 메모리에 대한 요청과 제한에 대한 별도의 합계를 정의한다.

즉, 각 개별 파드나 컨테이너에 개별적으로 적용하지 않고 모든 파드의 리소스 요청과 제한의 총합에 적용된다.

<br>

<img src="https://user-images.githubusercontent.com/37579681/126868275-0e60f8fc-f416-4f90-8922-8aa4f47c2b8a.jpeg">

<br>

***리소스쿼터와 함께 LimitRange 생성***

리소스쿼터를 생성할 때 주의할 점은 LimitRange 오브젝트도 함께 생성해야 한다는 것이다.

앞 절에서는 LimitRange를 설정했지만, 이를 생성하지 않았다면 어떤 리소스 요청이나 제한도 명시하지 않은 파드를 실행할 수 없게 된다.

**즉, 특정 리소스(CPU || MEM)에 대한 쿼터가 설정된 경우(요청 || 제한), 파드에는 동일한 리소스에 대한 요청 또는 제한이 각각 설정돼야 한다.**

<br>

---

<br>

### Persistent Storage에 관한 쿼터 지정

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: stroage
spec:
  hard:
    # 요청 가능한 스토리지의 전체 용량
    requests.storage: 500Gi
    # ssd 스토리지 클래스에서 요청 가능한 스토리지 용량
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti
```

PVC는 특정 스토리지 클래스에 동적 프로비저닝된 PV를 요청할 수 있다.

이것이 k8s가 각 스토리지 클래스에 개별적으로 스토리지 쿼터를 정의할 수 있게 한 이유다.

<br>

---

<br>

### 생성 가능한 오브젝트 수 제한

리소스쿼터는 네임스페이스 내의 파드, 레플리케이션컨트롤러, 서비스 및 그 외의 오브젝트 수를 제한하도록 구성할 수 있다.

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10
    replicationcontrollers: 5
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 4
    # 서비스 5개를 생성할 수 있고, 최대 1개의 LB 서비스와 최대 2개의 Nodeport 서비스가 될 수 있다.
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    ssd.storagecalss.storage.k8s.io/pertsistentvolumeclaims: 2
```

<br>

---

<br>

### 특정 파드 상태나 QoS 클래스에 대한 쿼터 지정

쿼터는 쿼터 범위로 제한될 수도 있다.

현재 BestEffort, NotBestEffort, Terminating, NotTerminating의 네 가지 범위를 사용할 수 있다.

처음 두 개는 이름에서부터 알 수 있듯 클래스 중 하나가 있는 파드에 적용되는지 여부를 결정한다.

다른 두 범위는 종료 과정 혹은 종료되지 않은 파드에 적용된다.

각 파드는 종료되고 실패로 표시되기 전에 얼마나 오래 실행할 수 있는지 activeDeadlineSeconds 필드를 설정하여 실패로 표시된 후 종료되기 전 시작 기간을 기준으로 노드에서 활성화되도록 허용할 수 있다.

Terminating 쿼터 범위는 activeDeadlineSeconds가 설정된 파드에 적용되는 반면 NotTerminating은 그렇지 않은 파드에 적용된다.

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:
  - BestEffort
  - NotTerminating
  hard:
    pods: 4
```

<br>

---
---

<br>

## 파드 리소스 사용량 모니터링

k8s 클러스터를 최대한 활용하기 위해 리소스 요청과 제한을 적절하게 설정하는 것이 매우 중요하다.

이런 적정 지점을 찾기 위해 애플리케이션이 사용자에게 노출된 후 계속 모니터링하고,

필요한 경우 리소스 요청과 제한을 조정해야 한다.

<br>

---

<br>

### 실제 리소스 사용량 수집과 검색

kubelet 자체에는 cAdvisor라는 에이전트가 포함되어 있는데 이 에이전트는 노드에서 실행되는 개별 컨테이너와 노드 전체의 리소스 사용 데이터를 수집한다.

전체 클러스터를 이러한 통계를 중앙에서 수집하려면 힙스터 혹은 메트릭서버 라는 추가 구성 요소를 실행해야 한다.